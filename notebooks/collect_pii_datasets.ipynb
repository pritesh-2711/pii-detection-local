{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PII Dataset Collection - Banking Domain\n",
    "\n",
    "Collects and inspects all PII/NER datasets relevant to banking.\n",
    "\n",
    "**Datasets covered:**\n",
    "1. `ai4privacy/pii-masking-400k`\n",
    "2. `ai4privacy/pii-masking-300k` (FinPII-80k split)\n",
    "3. `gretelai/synthetic_pii_finance_multilingual`\n",
    "4. `nvidia/Nemotron-PII`\n",
    "5. `wikiann` (en)\n",
    "6. `Babelscape/multinerd` (en)\n",
    "7. `DFKI-SLT/few-nerd`\n",
    "8. `conll2003`\n",
    "9. `nlpaueb/finer-139`\n",
    "10. `iiiorg/piiranha-v1-detect-personal-information`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/pritesh-jha/projects/pii-detection/pii-detection/notebooks/pii_datasets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = Path('./pii_datasets')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f'Output directory: {OUTPUT_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels_from_bio(dataset, label_field='ner_tags', label_names=None):\n",
    "    \"\"\"\n",
    "    Count unique entity types from a BIO-tagged HuggingFace dataset split.\n",
    "    Returns a set of entity type strings (without B-/I- prefix).\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    sample = dataset.select(range(min(500, len(dataset))))\n",
    "    for row in sample:\n",
    "        tags = row[label_field]\n",
    "        for tag in tags:\n",
    "            if isinstance(tag, int):\n",
    "                if label_names:\n",
    "                    tag = label_names[tag]\n",
    "                else:\n",
    "                    continue\n",
    "            if tag != 'O' and tag != '':\n",
    "                entity = tag.replace('B-', '').replace('I-', '').strip()\n",
    "                if entity:\n",
    "                    types.add(entity)\n",
    "    return types\n",
    "\n",
    "\n",
    "def save_split(dataset, name, split_name):\n",
    "    \"\"\"\n",
    "    Save a dataset split to disk as JSONL.\n",
    "    \"\"\"\n",
    "    out_path = OUTPUT_DIR / name\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "    filepath = out_path / f'{split_name}.jsonl'\n",
    "    dataset.to_json(str(filepath))\n",
    "    size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "    print(f'  Saved {split_name}: {len(dataset):,} rows -> {filepath} ({size_mb:.1f} MB)')\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def build_summary_row(name, url, license_, lang, num_rows, num_entity_types,\n",
    "                      entity_types, annotation_source, domain, banking_relevance, notes):\n",
    "    return {\n",
    "        'Dataset': name,\n",
    "        'URL': url,\n",
    "        'License': license_,\n",
    "        'Language(s)': lang,\n",
    "        'Total Rows': num_rows,\n",
    "        'PII Entity Types (#)': num_entity_types,\n",
    "        'Entity Types (sample)': entity_types,\n",
    "        'Annotation Source': annotation_source,\n",
    "        'Domain': domain,\n",
    "        'Banking Relevance': banking_relevance,\n",
    "        'Notes': notes\n",
    "    }\n",
    "\n",
    "\n",
    "summary_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1 — ai4privacy/pii-masking-400k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ai4privacy/pii-masking-400k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating train split: 100%|██████████| 325517/325517 [00:00<00:00, 407626.75 examples/s]\n",
      "Generating validation split: 100%|██████████| 81379/81379 [00:00<00:00, 455604.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source_text', 'locale', 'language', 'split', 'privacy_mask', 'uid', 'masked_text', 'mbert_tokens', 'mbert_token_classes'],\n",
      "        num_rows: 325517\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['source_text', 'locale', 'language', 'split', 'privacy_mask', 'uid', 'masked_text', 'mbert_tokens', 'mbert_token_classes'],\n",
      "        num_rows: 81379\n",
      "    })\n",
      "})\n",
      "  Rows: 325,517\n",
      "  Detected entity types (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 326/326 [00:03<00:00, 85.63ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 325,517 rows -> pii_datasets/ai4privacy_400k/train.jsonl (354.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading ai4privacy/pii-masking-400k ...')\n",
    "ds_400k = load_dataset('ai4privacy/pii-masking-400k')\n",
    "print(ds_400k)\n",
    "\n",
    "# Entity types come from bio_labels column\n",
    "train_split = ds_400k['train']\n",
    "bio_types = set()\n",
    "for row in train_split.select(range(min(1000, len(train_split)))):\n",
    "    for label in row.get('bio_labels', []):\n",
    "        if label != 'O':\n",
    "            bio_types.add(label.replace('B-', '').replace('I-', ''))\n",
    "\n",
    "print(f'  Rows: {len(train_split):,}')\n",
    "print(f'  Detected entity types ({len(bio_types)}): {sorted(bio_types)}')\n",
    "\n",
    "save_split(train_split, 'ai4privacy_400k', 'train')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='ai4privacy/pii-masking-400k',\n",
    "    url='https://huggingface.co/datasets/ai4privacy/pii-masking-400k',\n",
    "    license_='Custom (academic free, commercial needs license)',\n",
    "    lang='en, fr, de, it',\n",
    "    num_rows=len(train_split),\n",
    "    num_entity_types=63,\n",
    "    entity_types='FIRSTNAME, LASTNAME, EMAIL, PHONE, CREDITCARDNUMBER, SSN, IBAN, BITCOINADDRESS, ...',\n",
    "    annotation_source='Synthetic (proprietary algorithm)',\n",
    "    domain='General (business, education, psychology, legal)',\n",
    "    banking_relevance='High — covers IBAN, CREDITCARD, ACCOUNTNUMBER, BITCOINADDRESS',\n",
    "    notes='Latest version. 63 PII classes. Use FinPII split in 300k for finance-specific.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2 — ai4privacy/pii-masking-300k (FinPII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ai4privacy/pii-masking-300k ...\n",
      "  Available configs: ['default']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 177677/177677 [00:01<00:00, 163453.34 examples/s]\n",
      "Generating validation split: 100%|██████████| 47728/47728 [00:00<00:00, 154189.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
      "        num_rows: 177677\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
      "        num_rows: 47728\n",
      "    })\n",
      "})\n",
      "  Rows: 177,677\n",
      "  Detected entity types (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 178/178 [00:05<00:00, 35.53ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 177,677 rows -> pii_datasets/ai4privacy_300k/train.jsonl (552.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading ai4privacy/pii-masking-300k ...')\n",
    "# Check available configs\n",
    "try:\n",
    "    configs = get_dataset_config_names('ai4privacy/pii-masking-300k')\n",
    "    print(f'  Available configs: {configs}')\n",
    "except Exception:\n",
    "    configs = ['default']\n",
    "\n",
    "ds_300k = load_dataset('ai4privacy/pii-masking-300k')\n",
    "print(ds_300k)\n",
    "\n",
    "train_300k = ds_300k['train']\n",
    "bio_types_300k = set()\n",
    "for row in train_300k.select(range(min(1000, len(train_300k)))):\n",
    "    for label in row.get('bio_labels', []):\n",
    "        if label != 'O':\n",
    "            bio_types_300k.add(label.replace('B-', '').replace('I-', ''))\n",
    "\n",
    "print(f'  Rows: {len(train_300k):,}')\n",
    "print(f'  Detected entity types ({len(bio_types_300k)}): {sorted(bio_types_300k)}')\n",
    "\n",
    "save_split(train_300k, 'ai4privacy_300k', 'train')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='ai4privacy/pii-masking-300k',\n",
    "    url='https://huggingface.co/datasets/ai4privacy/pii-masking-300k',\n",
    "    license_='Custom (academic free, commercial needs license)',\n",
    "    lang='en, fr, de, it, es, pt',\n",
    "    num_rows=len(train_300k),\n",
    "    num_entity_types='27 (OpenPII) + ~20 (FinPII)',\n",
    "    entity_types='OpenPII-220k + FinPII-80k (finance/insurance-specific types)',\n",
    "    annotation_source='Synthetic + human-in-loop (~98.3% token accuracy)',\n",
    "    domain='General + Finance/Insurance (FinPII subset)',\n",
    "    banking_relevance='Very High — FinPII-80k explicitly targets finance/insurance',\n",
    "    notes='Best option for banking. FinPII contains ~20 finance-specific entity types.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3 — gretelai/synthetic_pii_finance_multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gretelai/synthetic_pii_finance_multilingual ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 50346/50346 [00:00<00:00, 379188.08 examples/s]\n",
      "Generating test split: 100%|██████████| 5594/5594 [00:00<00:00, 316613.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['level_0', 'index', 'document_type', 'document_description', 'expanded_type', 'expanded_description', 'language', 'language_description', 'domain', 'generated_text', 'pii_spans', 'conformance_score', 'quality_score', 'toxicity_score', 'bias_score', 'groundedness_score'],\n",
      "        num_rows: 50346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['level_0', 'index', 'document_type', 'document_description', 'expanded_type', 'expanded_description', 'language', 'language_description', 'domain', 'generated_text', 'pii_spans', 'conformance_score', 'quality_score', 'toxicity_score', 'bias_score', 'groundedness_score'],\n",
      "        num_rows: 5594\n",
      "    })\n",
      "})\n",
      "  Rows: 50,346\n",
      "  Columns: ['level_0', 'index', 'document_type', 'document_description', 'expanded_type', 'expanded_description', 'language', 'language_description', 'domain', 'generated_text', 'pii_spans', 'conformance_score', 'quality_score', 'toxicity_score', 'bias_score', 'groundedness_score']\n",
      "  Sample keys: ['level_0', 'index', 'document_type', 'document_description', 'expanded_type', 'expanded_description', 'language', 'language_description', 'domain', 'generated_text', 'pii_spans', 'conformance_score', 'quality_score', 'toxicity_score', 'bias_score', 'groundedness_score']\n",
      "  PII types found: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 51/51 [00:00<00:00, 117.01ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 50,346 rows -> pii_datasets/gretel_finance/train.jsonl (117.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 123.75ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved test: 5,594 rows -> pii_datasets/gretel_finance/test.jsonl (13.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading gretelai/synthetic_pii_finance_multilingual ...')\n",
    "ds_gretel = load_dataset('gretelai/synthetic_pii_finance_multilingual')\n",
    "print(ds_gretel)\n",
    "\n",
    "train_gretel = ds_gretel['train']\n",
    "print(f'  Rows: {len(train_gretel):,}')\n",
    "print(f'  Columns: {train_gretel.column_names}')\n",
    "\n",
    "# Inspect a sample to understand label format\n",
    "sample = train_gretel[0]\n",
    "print(f'  Sample keys: {list(sample.keys())}')\n",
    "\n",
    "# Get unique PII types from the dataset\n",
    "pii_types_gretel = set()\n",
    "label_col = None\n",
    "for col in ['pii_class', 'entity_type', 'label', 'ner_tags', 'labels']:\n",
    "    if col in train_gretel.column_names:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col:\n",
    "    for row in train_gretel.select(range(min(200, len(train_gretel)))):\n",
    "        val = row[label_col]\n",
    "        if isinstance(val, list):\n",
    "            for v in val:\n",
    "                pii_types_gretel.add(str(v))\n",
    "        else:\n",
    "            pii_types_gretel.add(str(val))\n",
    "\n",
    "print(f'  PII types found: {sorted(pii_types_gretel)}')\n",
    "\n",
    "save_split(train_gretel, 'gretel_finance', 'train')\n",
    "if 'test' in ds_gretel:\n",
    "    save_split(ds_gretel['test'], 'gretel_finance', 'test')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='gretelai/synthetic_pii_finance_multilingual',\n",
    "    url='https://huggingface.co/datasets/gretelai/synthetic_pii_finance_multilingual',\n",
    "    license_='Apache 2.0',\n",
    "    lang='en, es, sv, de, it, nl, fr',\n",
    "    num_rows=len(train_gretel),\n",
    "    num_entity_types=29,\n",
    "    entity_types='ACCOUNT_NUMBER, ROUTING_NUMBER, IBAN, CREDIT_CARD, SSN, TAX_ID, ...',\n",
    "    annotation_source='Synthetic (Gretel LLM + GLiNER validation + LLM-as-judge)',\n",
    "    domain='Finance (100 financial document types: bank statements, loan docs, wire transfers)',\n",
    "    banking_relevance='Very High — explicitly covers banking document formats',\n",
    "    notes='55,940 records. Avg doc length 1,357 chars. Best domain match for banking.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 4 — nvidia/Nemotron-PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nvidia/Nemotron-PII ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 100000/100000 [00:00<00:00, 263374.95 examples/s]\n",
      "Generating test split: 100%|██████████| 100000/100000 [00:00<00:00, 311244.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['uid', 'domain', 'document_type', 'document_description', 'document_format', 'locale', 'text', 'spans', 'text_tagged'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['uid', 'domain', 'document_type', 'document_description', 'document_format', 'locale', 'text', 'spans', 'text_tagged'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "})\n",
      "  Rows: 100,000\n",
      "  Columns: ['uid', 'domain', 'document_type', 'document_description', 'document_format', 'locale', 'text', 'spans', 'text_tagged']\n",
      "  Entity types: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 100/100 [00:01<00:00, 98.32ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 100,000 rows -> pii_datasets/nvidia_nemotron/train.jsonl (318.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading nvidia/Nemotron-PII ...')\n",
    "ds_nvidia = load_dataset('nvidia/Nemotron-PII')\n",
    "print(ds_nvidia)\n",
    "\n",
    "train_nvidia = ds_nvidia['train']\n",
    "print(f'  Rows: {len(train_nvidia):,}')\n",
    "print(f'  Columns: {train_nvidia.column_names}')\n",
    "\n",
    "# Get entity types\n",
    "nvidia_types = set()\n",
    "for row in train_nvidia.select(range(min(500, len(train_nvidia)))):\n",
    "    for col in ['ner_tags', 'labels', 'bio_labels', 'label']:\n",
    "        if col in row:\n",
    "            val = row[col]\n",
    "            if isinstance(val, list):\n",
    "                for v in val:\n",
    "                    tag = str(v)\n",
    "                    if tag != 'O':\n",
    "                        nvidia_types.add(tag.replace('B-', '').replace('I-', ''))\n",
    "            break\n",
    "\n",
    "print(f'  Entity types: {sorted(nvidia_types)}')\n",
    "\n",
    "save_split(train_nvidia, 'nvidia_nemotron', 'train')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='nvidia/Nemotron-PII',\n",
    "    url='https://huggingface.co/datasets/nvidia/Nemotron-PII',\n",
    "    license_='CC-BY 4.0',\n",
    "    lang='en',\n",
    "    num_rows=len(train_nvidia),\n",
    "    num_entity_types='55+',\n",
    "    entity_types='PII + PHI: names, SSN, DOB, ACCOUNT, DEVICE_ID, IP, BIOMETRIC, ...',\n",
    "    annotation_source='Synthetic (NVIDIA NeMo Data Designer, Census-grounded personas)',\n",
    "    domain='General across 50+ industries including finance',\n",
    "    banking_relevance='High — 50+ industries includes finance; covers PHI useful for KYC',\n",
    "    notes='100k records. Structured + unstructured docs. CC-BY 4.0 = commercial-friendly.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 5 — wikiann (en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wikiann (en) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 1628539.70 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 1936786.11 examples/s]\n",
      "Generating train split: 100%|██████████| 20000/20000 [00:00<00:00, 2366921.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n",
      "  Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
      "  Rows: 20,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 280.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 20,000 rows -> pii_datasets/wikiann/train.jsonl (3.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 293.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved validation: 10,000 rows -> pii_datasets/wikiann/validation.jsonl (1.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 291.45ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved test: 10,000 rows -> pii_datasets/wikiann/test.jsonl (1.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading wikiann (en) ...')\n",
    "ds_wikiann = load_dataset('wikiann', 'en')\n",
    "print(ds_wikiann)\n",
    "\n",
    "train_wiki = ds_wikiann['train']\n",
    "label_names = train_wiki.features['ner_tags'].feature.names\n",
    "print(f'  Labels: {label_names}')\n",
    "print(f'  Rows: {len(train_wiki):,}')\n",
    "\n",
    "save_split(train_wiki, 'wikiann', 'train')\n",
    "save_split(ds_wikiann['validation'], 'wikiann', 'validation')\n",
    "save_split(ds_wikiann['test'], 'wikiann', 'test')\n",
    "\n",
    "entity_types_wiki = set(l.replace('B-', '').replace('I-', '') for l in label_names if l != 'O')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='wikiann (en)',\n",
    "    url='https://huggingface.co/datasets/wikiann',\n",
    "    license_='CC-BY-SA 3.0',\n",
    "    lang='en (282 langs available)',\n",
    "    num_rows=len(train_wiki) + len(ds_wikiann['validation']) + len(ds_wikiann['test']),\n",
    "    num_entity_types=len(entity_types_wiki),\n",
    "    entity_types=', '.join(sorted(entity_types_wiki)),\n",
    "    annotation_source='Auto-annotated from Wikipedia using cross-lingual projection',\n",
    "    domain='General (Wikipedia articles)',\n",
    "    banking_relevance='Medium — PER/ORG/LOC only; good for entity grounding in text',\n",
    "    notes='Only 3 entity types. Use as supplementary data for PER/ORG/LOC coverage.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 6 — Babelscape/multinerd (en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Babelscape/multinerd ...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'lang'],\n",
      "        num_rows: 1339200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'lang'],\n",
      "        num_rows: 167400\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'lang'],\n",
      "        num_rows: 167993\n",
      "    })\n",
      "})\n",
      "  Labels: ['ANIM', 'BIO', 'CEL', 'DIS', 'EVE', 'FOOD', 'INST', 'LOC', 'MEDIA', 'MYTH', 'ORG', 'PER', 'PLANT', 'TIME', 'VEHI']\n",
      "  EN rows: 131,280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 132/132 [00:00<00:00, 154.02ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train_en: 131,280 rows -> pii_datasets/multinerd/train_en.jsonl (30.4 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading Babelscape/multinerd ...')\n",
    "ds_multinerd = load_dataset('Babelscape/multinerd', verification_mode='no_checks')\n",
    "print(ds_multinerd)\n",
    "\n",
    "train_mn = ds_multinerd['train']\n",
    "\n",
    "# Filter English only\n",
    "if 'lang' in train_mn.column_names:\n",
    "    train_mn_en = train_mn.filter(lambda x: x['lang'] == 'en')\n",
    "else:\n",
    "    train_mn_en = train_mn\n",
    "\n",
    "# ner_tags in this dataset is a Sequence of Value(int64), not ClassLabel.\n",
    "# The integer-to-label mapping is documented in the dataset card.\n",
    "multinerd_id2label = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER', 2: 'I-PER',\n",
    "    3: 'B-ORG', 4: 'I-ORG',\n",
    "    5: 'B-LOC', 6: 'I-LOC',\n",
    "    7: 'B-ANIM', 8: 'I-ANIM',\n",
    "    9: 'B-BIO', 10: 'I-BIO',\n",
    "    11: 'B-CEL', 12: 'I-CEL',\n",
    "    13: 'B-DIS', 14: 'I-DIS',\n",
    "    15: 'B-EVE', 16: 'I-EVE',\n",
    "    17: 'B-FOOD', 18: 'I-FOOD',\n",
    "    19: 'B-INST', 20: 'I-INST',\n",
    "    21: 'B-MEDIA', 22: 'I-MEDIA',\n",
    "    23: 'B-MYTH', 24: 'I-MYTH',\n",
    "    25: 'B-PLANT', 26: 'I-PLANT',\n",
    "    27: 'B-TIME', 28: 'I-TIME',\n",
    "    29: 'B-VEHI', 30: 'I-VEHI',\n",
    "}\n",
    "entity_types_mn = set(\n",
    "    v.replace('B-', '').replace('I-', '')\n",
    "    for v in multinerd_id2label.values() if v != 'O'\n",
    ")\n",
    "print(f'  Labels: {sorted(entity_types_mn)}')\n",
    "print(f'  EN rows: {len(train_mn_en):,}')\n",
    "\n",
    "save_split(train_mn_en, 'multinerd', 'train_en')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='Babelscape/multinerd',\n",
    "    url='https://huggingface.co/datasets/Babelscape/multinerd',\n",
    "    license_='CC-BY-NC-SA 4.0',\n",
    "    lang='en, de, es, fr, it, nl, pl, pt, ru, zh',\n",
    "    num_rows=len(train_mn_en),\n",
    "    num_entity_types=len(entity_types_mn),\n",
    "    entity_types=', '.join(sorted(entity_types_mn)),\n",
    "    annotation_source='Expert-annotated',\n",
    "    domain='General (Wikipedia + news)',\n",
    "    banking_relevance='Medium — PER/ORG/LOC plus TIME/EVE useful for transaction context',\n",
    "    notes='15 types. NC license — not for commercial use as-is.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 7 — DFKI-SLT/few-nerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DFKI-SLT/few-nerd (supervised split) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 131767/131767 [00:00<00:00, 753340.40 examples/s]\n",
      "Generating validation split: 100%|██████████| 18824/18824 [00:00<00:00, 855577.84 examples/s]\n",
      "Generating test split: 100%|██████████| 37648/37648 [00:00<00:00, 1142251.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
      "        num_rows: 131767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
      "        num_rows: 18824\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
      "        num_rows: 37648\n",
      "    })\n",
      "})\n",
      "  Rows: 131,767\n",
      "  Entity types (8): ['art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 132/132 [00:00<00:00, 189.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 131,767 rows -> pii_datasets/few_nerd/train.jsonl (43.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 19/19 [00:00<00:00, 160.81ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved validation: 18,824 rows -> pii_datasets/few_nerd/validation.jsonl (6.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 38/38 [00:00<00:00, 186.07ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved test: 37,648 rows -> pii_datasets/few_nerd/test.jsonl (12.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading DFKI-SLT/few-nerd (supervised split) ...')\n",
    "ds_fewnerd = load_dataset('DFKI-SLT/few-nerd', 'supervised')\n",
    "print(ds_fewnerd)\n",
    "\n",
    "train_fn = ds_fewnerd['train']\n",
    "label_names_fn = train_fn.features['ner_tags'].feature.names\n",
    "\n",
    "# Get unique types from sample\n",
    "fn_types = set()\n",
    "for row in train_fn.select(range(min(500, len(train_fn)))):\n",
    "    for tag_id in row['ner_tags']:\n",
    "        label = label_names_fn[tag_id]\n",
    "        if label != 'O':\n",
    "            fn_types.add(label.replace('B-', '').replace('I-', ''))\n",
    "\n",
    "print(f'  Rows: {len(train_fn):,}')\n",
    "print(f'  Entity types ({len(fn_types)}): {sorted(fn_types)[:20]} ...')\n",
    "\n",
    "save_split(train_fn, 'few_nerd', 'train')\n",
    "save_split(ds_fewnerd['validation'], 'few_nerd', 'validation')\n",
    "save_split(ds_fewnerd['test'], 'few_nerd', 'test')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='DFKI-SLT/few-nerd',\n",
    "    url='https://huggingface.co/datasets/DFKI-SLT/few-nerd',\n",
    "    license_='CC-BY-SA 4.0',\n",
    "    lang='en',\n",
    "    num_rows=len(train_fn) + len(ds_fewnerd['validation']) + len(ds_fewnerd['test']),\n",
    "    num_entity_types=66,\n",
    "    entity_types='person-politician, org-company, location-city, product-software, ... (66 fine-grained)',\n",
    "    annotation_source='Crowdsourced (188k sentences)',\n",
    "    domain='General (Wikipedia)',\n",
    "    banking_relevance='Low-Medium — org-company, location useful; no direct PII types',\n",
    "    notes='Fine-grained NER. Useful for entity disambiguation, not direct PII tagging.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 8 — CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conll2003 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 14041 examples [00:00, 689451.09 examples/s]\n",
      "Generating validation split: 3250 examples [00:00, 411877.21 examples/s]\n",
      "Generating test split: 3453 examples [00:00, 504491.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "  Rows: 14,041\n",
      "  Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 15/15 [00:00<00:00, 194.50ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 14,041 rows -> pii_datasets/conll2003/train.jsonl (3.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 242.61ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved validation: 3,250 rows -> pii_datasets/conll2003/validation.jsonl (0.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 261.16ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved test: 3,453 rows -> pii_datasets/conll2003/test.jsonl (0.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading conll2003 ...')\n",
    "# conll2003 uses a legacy .py loading script blocked in datasets>=4.0.\n",
    "# Use the auto-converted Parquet revision instead.\n",
    "ds_conll = load_dataset('conll2003', revision='refs/convert/parquet')\n",
    "print(ds_conll)\n",
    "\n",
    "train_conll = ds_conll['train']\n",
    "label_names_conll = train_conll.features['ner_tags'].feature.names\n",
    "entity_types_conll = set(l.replace('B-', '').replace('I-', '') for l in label_names_conll if l != 'O')\n",
    "\n",
    "print(f'  Rows: {len(train_conll):,}')\n",
    "print(f'  Labels: {label_names_conll}')\n",
    "\n",
    "save_split(train_conll, 'conll2003', 'train')\n",
    "save_split(ds_conll['validation'], 'conll2003', 'validation')\n",
    "save_split(ds_conll['test'], 'conll2003', 'test')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='conll2003',\n",
    "    url='https://huggingface.co/datasets/conll2003',\n",
    "    license_='Custom (non-commercial research)',\n",
    "    lang='en',\n",
    "    num_rows=len(train_conll) + len(ds_conll['validation']) + len(ds_conll['test']),\n",
    "    num_entity_types=len(entity_types_conll),\n",
    "    entity_types=', '.join(sorted(entity_types_conll)),\n",
    "    annotation_source='Expert-annotated (newswire)',\n",
    "    domain='News (Reuters 1996)',\n",
    "    banking_relevance='Low — only PER/ORG/LOC/MISC; no financial PII',\n",
    "    notes='Industry baseline. Already in your repo. Non-commercial license.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 9 — nlpaueb/finer-139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nlpaueb/finer-139 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 900384 examples [00:01, 455939.64 examples/s]\n",
      "Generating validation split: 112494 examples [00:00, 358829.62 examples/s]\n",
      "Generating test split: 108378 examples [00:00, 320540.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 900384\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 112494\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 108378\n",
      "    })\n",
      "})\n",
      "  Rows: 900,384\n",
      "  Entity type count: 279 labels (64 found in sample)\n",
      "  Sample types: ['AllocatedShareBasedCompensationExpense', 'AmortizationOfFinancingCosts', 'AmortizationOfIntangibleAssets', 'AntidilutiveSecuritiesExcludedFromComputationOfEarningsPerShareAmount', 'AreaOfRealEstateProperty', 'BusinessAcquisitionPercentageOfVotingInterestsAcquired', 'BusinessCombinationConsiderationTransferred1', 'ClassOfWarrantOrRightExercisePriceOfWarrantsOrRights1', 'CommonStockCapitalSharesReservedForFutureIssuance', 'CommonStockDividendsPerShareDeclared']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 901/901 [00:05<00:00, 169.89ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 900,384 rows -> pii_datasets/finer_139/train.jsonl (409.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 113/113 [00:00<00:00, 170.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved validation: 112,494 rows -> pii_datasets/finer_139/validation.jsonl (52.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 109/109 [00:00<00:00, 162.32ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved test: 108,378 rows -> pii_datasets/finer_139/test.jsonl (51.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading nlpaueb/finer-139 ...')\n",
    "# finer-139 also uses a legacy .py script. Use the Parquet revision.\n",
    "ds_finer = load_dataset('nlpaueb/finer-139', revision='refs/convert/parquet')\n",
    "print(ds_finer)\n",
    "\n",
    "train_finer = ds_finer['train']\n",
    "label_names_finer = train_finer.features['ner_tags'].feature.names\n",
    "\n",
    "# Sample a few entity types\n",
    "finer_types = set()\n",
    "for row in train_finer.select(range(min(1000, len(train_finer)))):\n",
    "    for tag_id in row['ner_tags']:\n",
    "        label = label_names_finer[tag_id]\n",
    "        if label != 'O':\n",
    "            finer_types.add(label.replace('B-', '').replace('I-', ''))\n",
    "\n",
    "print(f'  Rows: {len(train_finer):,}')\n",
    "print(f'  Entity type count: {len(label_names_finer)} labels ({len(finer_types)} found in sample)')\n",
    "print(f'  Sample types: {sorted(list(finer_types))[:10]}')\n",
    "\n",
    "save_split(train_finer, 'finer_139', 'train')\n",
    "save_split(ds_finer['validation'], 'finer_139', 'validation')\n",
    "save_split(ds_finer['test'], 'finer_139', 'test')\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='nlpaueb/finer-139',\n",
    "    url='https://huggingface.co/datasets/nlpaueb/finer-139',\n",
    "    license_='CC-BY-SA 4.0',\n",
    "    lang='en',\n",
    "    num_rows=len(train_finer) + len(ds_finer['validation']) + len(ds_finer['test']),\n",
    "    num_entity_types=139,\n",
    "    entity_types='XBRL financial tags: Revenue, Assets, LiabilitiesTotal, DebtCurrent, EPS, ...',\n",
    "    annotation_source='Expert-annotated (SEC professional auditors via EDGAR filings)',\n",
    "    domain='Finance (SEC 10-K/10-Q annual/quarterly reports)',\n",
    "    banking_relevance='High for financial domain language; not classical PII but financial entities',\n",
    "    notes='1.1M sentences. Use for continued pre-training on financial text, not PII labels directly.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 10 — iiiorg/piiranha-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Isotonic/pii-masking-200k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 209261/209261 [00:00<00:00, 287489.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['masked_text', 'unmasked_text', 'privacy_mask', 'span_labels', 'bio_labels', 'tokenised_text', 'language'],\n",
      "        num_rows: 209261\n",
      "    })\n",
      "})\n",
      "  Rows: 209,261\n",
      "  Columns: ['masked_text', 'unmasked_text', 'privacy_mask', 'span_labels', 'bio_labels', 'tokenised_text', 'language']\n",
      "  Entity types (56): ['ACCOUNTNAME', 'ACCOUNTNUMBER', 'AGE', 'AMOUNT', 'BIC', 'BITCOINADDRESS', 'BUILDINGNUMBER', 'CITY', 'COMPANYNAME', 'COUNTY', 'CREDITCARDCVV', 'CREDITCARDISSUER', 'CREDITCARDNUMBER', 'CURRENCY', 'CURRENCYCODE', 'CURRENCYNAME', 'CURRENCYSYMBOL', 'DATE', 'DOB', 'EMAIL', 'ETHEREUMADDRESS', 'EYECOLOR', 'FIRSTNAME', 'GENDER', 'HEIGHT', 'IBAN', 'IP', 'IPV4', 'IPV6', 'JOBAREA', 'JOBTITLE', 'JOBTYPE', 'LASTNAME', 'LITECOINADDRESS', 'MAC', 'MASKEDNUMBER', 'MIDDLENAME', 'NEARBYGPSCOORDINATE', 'ORDINALDIRECTION', 'PASSWORD', 'PHONEIMEI', 'PHONENUMBER', 'PIN', 'PREFIX', 'SECONDARYADDRESS', 'SEX', 'SSN', 'STATE', 'STREET', 'TIME', 'URL', 'USERAGENT', 'USERNAME', 'VEHICLEVIN', 'VEHICLEVRM', 'ZIPCODE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 210/210 [00:02<00:00, 84.13ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved train: 209,261 rows -> pii_datasets/isotonic_pii_200k/train.jsonl (302.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ai4privacy/pii-masking-43k has a malformed CSV on the Hub (ParserError at line 42759).\n",
    "# Replacing with Isotonic/pii-masking-200k — a clean Parquet mirror of the same data\n",
    "# with identical schema, confirmed loadable.\n",
    "print('Loading Isotonic/pii-masking-200k ...')\n",
    "ds_iso = load_dataset('Isotonic/pii-masking-200k')\n",
    "print(ds_iso)\n",
    "\n",
    "split_key = list(ds_iso.keys())[0]\n",
    "split_iso = ds_iso[split_key]\n",
    "print(f'  Rows: {len(split_iso):,}')\n",
    "print(f'  Columns: {split_iso.column_names}')\n",
    "\n",
    "bio_types_iso = set()\n",
    "for row in split_iso.select(range(min(500, len(split_iso)))):\n",
    "    for label in row.get('bio_labels', []):\n",
    "        if label != 'O':\n",
    "            bio_types_iso.add(label.replace('B-', '').replace('I-', ''))\n",
    "\n",
    "print(f'  Entity types ({len(bio_types_iso)}): {sorted(bio_types_iso)}')\n",
    "\n",
    "save_split(split_iso, 'isotonic_pii_200k', split_key)\n",
    "\n",
    "summary_rows.append(build_summary_row(\n",
    "    name='Isotonic/pii-masking-200k',\n",
    "    url='https://huggingface.co/datasets/Isotonic/pii-masking-200k',\n",
    "    license_='Apache 2.0',\n",
    "    lang='en, fr, de, it',\n",
    "    num_rows=len(split_iso),\n",
    "    num_entity_types=len(bio_types_iso) if bio_types_iso else 54,\n",
    "    entity_types=', '.join(sorted(bio_types_iso)) if bio_types_iso else 'Same 54 classes as ai4privacy series',\n",
    "    annotation_source='Synthetic (ai4privacy pipeline)',\n",
    "    domain='General',\n",
    "    banking_relevance='Medium — same PII classes as ai4privacy series; clean Parquet format',\n",
    "    notes='Clean mirror of ai4privacy/pii-masking-200k. Apache 2.0 license. Used as eval benchmark in research.'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DATASET SUMMARY =====\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| Dataset                                     | URL                                                                         | License                                          | Language(s)                            |   Total Rows | PII Entity Types (#)        | Entity Types (sample)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Annotation Source                                               | Domain                                                                             | Banking Relevance                                                            | Notes                                                                                                |\n",
      "+=============================================+=============================================================================+==================================================+========================================+==============+=============================+========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+=================================================================+====================================================================================+==============================================================================+======================================================================================================+\n",
      "| ai4privacy/pii-masking-400k                 | https://huggingface.co/datasets/ai4privacy/pii-masking-400k                 | Custom (academic free, commercial needs license) | en, fr, de, it                         |       325517 | 63                          | FIRSTNAME, LASTNAME, EMAIL, PHONE, CREDITCARDNUMBER, SSN, IBAN, BITCOINADDRESS, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Synthetic (proprietary algorithm)                               | General (business, education, psychology, legal)                                   | High — covers IBAN, CREDITCARD, ACCOUNTNUMBER, BITCOINADDRESS                | Latest version. 63 PII classes. Use FinPII split in 300k for finance-specific.                       |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| ai4privacy/pii-masking-300k                 | https://huggingface.co/datasets/ai4privacy/pii-masking-300k                 | Custom (academic free, commercial needs license) | en, fr, de, it, es, pt                 |       177677 | 27 (OpenPII) + ~20 (FinPII) | OpenPII-220k + FinPII-80k (finance/insurance-specific types)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Synthetic + human-in-loop (~98.3% token accuracy)               | General + Finance/Insurance (FinPII subset)                                        | Very High — FinPII-80k explicitly targets finance/insurance                  | Best option for banking. FinPII contains ~20 finance-specific entity types.                          |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| gretelai/synthetic_pii_finance_multilingual | https://huggingface.co/datasets/gretelai/synthetic_pii_finance_multilingual | Apache 2.0                                       | en, es, sv, de, it, nl, fr             |        50346 | 29                          | ACCOUNT_NUMBER, ROUTING_NUMBER, IBAN, CREDIT_CARD, SSN, TAX_ID, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Synthetic (Gretel LLM + GLiNER validation + LLM-as-judge)       | Finance (100 financial document types: bank statements, loan docs, wire transfers) | Very High — explicitly covers banking document formats                       | 55,940 records. Avg doc length 1,357 chars. Best domain match for banking.                           |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| nvidia/Nemotron-PII                         | https://huggingface.co/datasets/nvidia/Nemotron-PII                         | CC-BY 4.0                                        | en                                     |       100000 | 55+                         | PII + PHI: names, SSN, DOB, ACCOUNT, DEVICE_ID, IP, BIOMETRIC, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Synthetic (NVIDIA NeMo Data Designer, Census-grounded personas) | General across 50+ industries including finance                                    | High — 50+ industries includes finance; covers PHI useful for KYC            | 100k records. Structured + unstructured docs. CC-BY 4.0 = commercial-friendly.                       |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| wikiann (en)                                | https://huggingface.co/datasets/wikiann                                     | CC-BY-SA 3.0                                     | en (282 langs available)               |        40000 | 3                           | LOC, ORG, PER                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Auto-annotated from Wikipedia using cross-lingual projection    | General (Wikipedia articles)                                                       | Medium — PER/ORG/LOC only; good for entity grounding in text                 | Only 3 entity types. Use as supplementary data for PER/ORG/LOC coverage.                             |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| DFKI-SLT/few-nerd                           | https://huggingface.co/datasets/DFKI-SLT/few-nerd                           | CC-BY-SA 4.0                                     | en                                     |       188239 | 66                          | person-politician, org-company, location-city, product-software, ... (66 fine-grained)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Crowdsourced (188k sentences)                                   | General (Wikipedia)                                                                | Low-Medium — org-company, location useful; no direct PII types               | Fine-grained NER. Useful for entity disambiguation, not direct PII tagging.                          |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| conll2003                                   | https://huggingface.co/datasets/conll2003                                   | Custom (non-commercial research)                 | en                                     |        20744 | 4                           | LOC, MISC, ORG, PER                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Expert-annotated (newswire)                                     | News (Reuters 1996)                                                                | Low — only PER/ORG/LOC/MISC; no financial PII                                | Industry baseline. Already in your repo. Non-commercial license.                                     |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| nlpaueb/finer-139                           | https://huggingface.co/datasets/nlpaueb/finer-139                           | CC-BY-SA 4.0                                     | en                                     |      1121256 | 139                         | XBRL financial tags: Revenue, Assets, LiabilitiesTotal, DebtCurrent, EPS, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Expert-annotated (SEC professional auditors via EDGAR filings)  | Finance (SEC 10-K/10-Q annual/quarterly reports)                                   | High for financial domain language; not classical PII but financial entities | 1.1M sentences. Use for continued pre-training on financial text, not PII labels directly.           |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| Babelscape/multinerd                        | https://huggingface.co/datasets/Babelscape/multinerd                        | CC-BY-NC-SA 4.0                                  | en, de, es, fr, it, nl, pl, pt, ru, zh |       131280 | 15                          | ANIM, BIO, CEL, DIS, EVE, FOOD, INST, LOC, MEDIA, MYTH, ORG, PER, PLANT, TIME, VEHI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Expert-annotated                                                | General (Wikipedia + news)                                                         | Medium — PER/ORG/LOC plus TIME/EVE useful for transaction context            | 15 types. NC license — not for commercial use as-is.                                                 |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "| Isotonic/pii-masking-200k                   | https://huggingface.co/datasets/Isotonic/pii-masking-200k                   | Apache 2.0                                       | en, fr, de, it                         |       209261 | 56                          | ACCOUNTNAME, ACCOUNTNUMBER, AGE, AMOUNT, BIC, BITCOINADDRESS, BUILDINGNUMBER, CITY, COMPANYNAME, COUNTY, CREDITCARDCVV, CREDITCARDISSUER, CREDITCARDNUMBER, CURRENCY, CURRENCYCODE, CURRENCYNAME, CURRENCYSYMBOL, DATE, DOB, EMAIL, ETHEREUMADDRESS, EYECOLOR, FIRSTNAME, GENDER, HEIGHT, IBAN, IP, IPV4, IPV6, JOBAREA, JOBTITLE, JOBTYPE, LASTNAME, LITECOINADDRESS, MAC, MASKEDNUMBER, MIDDLENAME, NEARBYGPSCOORDINATE, ORDINALDIRECTION, PASSWORD, PHONEIMEI, PHONENUMBER, PIN, PREFIX, SECONDARYADDRESS, SEX, SSN, STATE, STREET, TIME, URL, USERAGENT, USERNAME, VEHICLEVIN, VEHICLEVRM, ZIPCODE | Synthetic (ai4privacy pipeline)                                 | General                                                                            | Medium — same PII classes as ai4privacy series; clean Parquet format         | Clean mirror of ai4privacy/pii-masking-200k. Apache 2.0 license. Used as eval benchmark in research. |\n",
      "+---------------------------------------------+-----------------------------------------------------------------------------+--------------------------------------------------+----------------------------------------+--------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Summary saved to: pii_datasets/dataset_summary.csv\n"
     ]
    }
   ],
   "source": [
    "df_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Display full table\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print('\\n===== DATASET SUMMARY =====')\n",
    "print(tabulate(df_summary, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = OUTPUT_DIR / 'dataset_summary.csv'\n",
    "df_summary.to_csv(csv_path, index=False)\n",
    "print(f'\\nSummary saved to: {csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Verify saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FILES ON DISK =====\n",
      "File                             Size (MB)\n",
      "-----------------------------  -----------\n",
      "ai4privacy_300k/train.jsonl          552.3\n",
      "ai4privacy_400k/train.jsonl          354.2\n",
      "conll2003/test.jsonl                   0.9\n",
      "conll2003/train.jsonl                  3.8\n",
      "conll2003/validation.jsonl             0.9\n",
      "few_nerd/test.jsonl                   12.3\n",
      "few_nerd/train.jsonl                  43.3\n",
      "few_nerd/validation.jsonl              6.2\n",
      "finer_139/test.jsonl                  51.3\n",
      "finer_139/train.jsonl                409\n",
      "finer_139/validation.jsonl            52.1\n",
      "gretel_finance/test.jsonl             13.2\n",
      "gretel_finance/train.jsonl           117.9\n",
      "isotonic_pii_200k/train.jsonl        302.1\n",
      "multinerd/train_en.jsonl              30.4\n",
      "nvidia_nemotron/train.jsonl          318.6\n",
      "wikiann/test.jsonl                     1.9\n",
      "wikiann/train.jsonl                    3.8\n",
      "wikiann/validation.jsonl               1.9\n",
      "\n",
      "Total disk usage: 2276.0 MB\n",
      "Summary CSV: /home/pritesh-jha/projects/pii-detection/pii-detection/notebooks/pii_datasets/dataset_summary.csv\n"
     ]
    }
   ],
   "source": [
    "print('\\n===== FILES ON DISK =====')\n",
    "total_size = 0\n",
    "file_rows = []\n",
    "\n",
    "for jsonl_file in sorted(OUTPUT_DIR.rglob('*.jsonl')):\n",
    "    size_mb = jsonl_file.stat().st_size / (1024 * 1024)\n",
    "    total_size += size_mb\n",
    "    file_rows.append({'File': str(jsonl_file.relative_to(OUTPUT_DIR)), 'Size (MB)': f'{size_mb:.1f}'})\n",
    "\n",
    "print(tabulate(file_rows, headers='keys', tablefmt='simple'))\n",
    "print(f'\\nTotal disk usage: {total_size:.1f} MB')\n",
    "print(f'Summary CSV: {(OUTPUT_DIR / \"dataset_summary.csv\").resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
